---
title: "Time Series Analysis of Nintendo stock price"
output: html_document
---
### 1. Introduction

```{r}
library(pomp)
library(plyr)
library(ggplot2)
library(foreach)
library(tseries)
```
Stock Price is of a big interest to many people. When analysing its property, the returns are often used instead of the original prices. This is because stock returns are often found to be uncorrelated. Moreover, because of one characteristic of market index called volatility, which is the degree of variation of returns, a simple time series model such as ARMA model may not be good enough to capture this property. Thus, more advanced models may be needed.

In this project, I study the time series of Nintendo Stock Price. I will first fit a ARIMA model, then follow with a POMP (partially observed Markov process) model and a GARCH model.

### 2. Explore the Data

The data is from [Yahoo](https://finance.yahoo.com/quote/NTDOY/history?period1=1208836800&period2=1524369600&interval=1wk&filter=history&frequency=1wk). It is the weekly data contains 523 records from 4/22/2008 to 4/22/2018.
```{r}
dat=read.csv(file="NTDOY.csv",header=TRUE)
head(dat)
```
Here I will use the adjusted close price. Let look at the plot of the data:

```{r}
ntd=dat$Adj.Close
ntd_ts=ts(ntd,start=2008.17,frequency = 52)
plot(ntd_ts,type='l',ylab='Nintendo Stock Price',main='Nintendo Stock Price')
```
As we can see, the Nintendo's stock price is at a peak at around 2008 mainly due to the success of its console Wii released in 2006. It began to decrease since then and went to a trough between 2012 and 2016. This corresponded to the failure of its new console after Wii called Wii U that is released in 2012 and the popularity of PS4 and Xbox One released in 2013. After 2017, we can see the stock price in general kept increasing, resulting from the huge success of Nintendo's new console Switch launched in 2017.

Let $\{z^*_n, n=1...N\}$ denote the data, then we write the log return $y^*_n$ as 
$$y^*_n=log(z^*_n)-log(z^*_{n-1})$$
The plot is below:
```{r}
ntd_df=diff(log(ntd))
plot(ntd_df,type='l',xlab='Time',main='Nintendo Log Return')
```

```{r}
acf(ntd_df, main='Acf of Log Returns')
```

From the ACF, we see the log returns are uncorrelated since they all lie in the confidence interval.

And we are ready to fit models.

### 3. ARMA Model

```{r echo=FALSE}
spectrum(ntd_df, spans=c(3,5),main='Smoothed Periodogram')
```
From the periodogram, there is a high peak at frequency = 0.44, which suggests a period of 2.3 weeks. Thus there is a seanality the may take into consideration. 

The trend is not clear to observe, so let's decompress the returns to investigate.
```{r}
de=decompose(ts(ntd_df,start=2008.17,frequency = 52))
plot(de)
```
The decompose shows that there is a positive trending to the returns, and the seaonal behavior is also obvious. 

So let's look at the random part and try to fit it into an ARMA model based on AIC values:

```{r echo=FALSE,warning=FALSE}
aic_table = function(data,P,Q){
  table = matrix(NA,(P+1),(Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
      table[p+1,q+1] = arima(data,order=c(p,0,q))$aic
    }
  }
  dimnames(table)=list(paste("AR",0:P,sep=""),paste("MA",0:Q,sep=""))
  table
}
d_aic_table = aic_table(de$random,5,5)
require(knitr)
kable(d_aic_table,digits=2)
```
We choose ARMA(0,0) that has a low AIC, so we choose to fit this model. The summary is below:
```{r}
ar=arima(de$random,order = c(0,0,0))
ar
```
Next we look at the residuals of the mp=odel:
```{r}
qqnorm(ar$residuals)
qqline(ar$residuals)
```

```{r}
shapiro.test(ar$residuals)
```
The normal QQ-plot shows heavy tails on both end. The Shapiro-Wilk normality test also rejects the assumption that the residuals are normally distrubuted. 

Thus, this is not a good model.


### 4. POMP Model

The phenomenon that negative shocks to a stockmarket index are associated with a subsequent increase in volatility is called leverage.

We define leverage $R_n$ on day n as the correlation between return on dat n-1 and the increase in the log volatility from day n-1 to day n. According to Bret¨®, $R_n$ is a random walk:
$$R_n=\frac{\{exp2G_n\}-1}{\{exp2G_n\}+1}$$
where $\{G_n\}$ is Gaussian random walk.

Then the model is :
$$Y_n=exp\{H_n/2\}$$
$$H_n=\mu_h(1-\phi)+\phi H_{n-1}+\beta_{n-1}R_n exp\{-H_{n-1}/2\}+\omega_n$$
$$G_n=G_{n-1}+\nu_n$$
where $\beta=Y_n\sigma_\eta\sqrt(1-\phi^2)$, $\{\epsilon\}$ is iid N(0,1), $\{\nu_n\}$ is iid N(0,$\sigma^2_\nu$), $\{\omega_n\}$ is N(0,$\sigma^2_\omega$), H_n is log volatility.

### 4.1 Building a POMP object

Then I build a POMP project:

```{r}
ntd_statenames <- c("H","G","Y_state")
ntd_rp_names <- c("sigma_nu","mu_h","phi","sigma_eta")
ntd_ivp_names <- c("G_0","H_0")
ntd_paramnames <- c(ntd_rp_names,ntd_ivp_names)
ntd_covarnames <- "covaryt"
```

```{r}
rproc1 <- "
double beta,omega,nu;
omega = rnorm(0,sigma_eta * sqrt( 1- phi*phi ) * sqrt(1-tanh(G)*tanh(G)));
nu = rnorm(0, sigma_nu);
G += nu;
beta = Y_state * sigma_eta * sqrt( 1- phi*phi );
H = mu_h*(1 - phi) + phi*H + beta * tanh( G ) * exp(-H/2) + omega;
"
rproc2.sim <- "
Y_state = rnorm( 0,exp(H/2) );
"

rproc2.filt <- "
Y_state = covaryt;
"
ntd_rproc.sim <- paste(rproc1,rproc2.sim)
ntd_rproc.filt <- paste(rproc1,rproc2.filt)

```

```{r}
ntd_initializer <- "
G = G_0;
H = H_0;
Y_state = rnorm( 0,exp(H/2) );
"
```

```{r}
ntd_rmeasure <- "
y=Y_state;
"

ntd_dmeasure <- "
lik=dnorm(y,0,exp(H/2),give_log);
"
```

```{r}
ntd_toEstimationScale <- "
Tsigma_eta = log(sigma_eta);
Tsigma_nu = log(sigma_nu);
Tphi = logit(phi);
"

ntd_fromEstimationScale <- "
Tsigma_eta = exp(sigma_eta);
Tsigma_nu = exp(sigma_nu);
Tphi = expit(phi);
"
```
 
 Then I simulate with an arbitrary parameters.
```{r}
ntd.filt <- pomp(data=data.frame(y=ntd_df,
                                 time=1:length(ntd_df)),
                 statenames=ntd_statenames,
                 paramnames=ntd_paramnames,
                 covarnames=ntd_covarnames,
                 times="time",
                 t0=0,
                 covar=data.frame(covaryt=c(0,ntd_df),
                                  time=0:length(ntd_df)),
                 tcovar="time",
                 rmeasure=Csnippet(ntd_rmeasure),
                 dmeasure=Csnippet(ntd_dmeasure),
                 rprocess=discrete.time.sim(step.fun=Csnippet(ntd_rproc.filt),delta.t=1),
                 initializer=Csnippet(ntd_initializer),
                 toEstimationScale=Csnippet(ntd_toEstimationScale), 
                 fromEstimationScale=Csnippet(ntd_fromEstimationScale)
)


expit<-function(real){1/(1+exp(-real))}
logit<-function(p.arg){log(p.arg/(1-p.arg))}
params_test <- c(
  sigma_nu = exp(-4.5),  
  mu_h = -0.25,       
  phi = expit(4),     
  sigma_eta = exp(-0.07),
  G_0 = 0,
  H_0=0
)
sim1.sim <- pomp(ntd.filt, 
                 statenames=ntd_statenames,
                 paramnames=ntd_paramnames,
                 covarnames=ntd_covarnames,
                 rprocess=discrete.time.sim(step.fun=Csnippet(ntd_rproc.sim),delta.t=1)
)

sim1.sim <- simulate(sim1.sim,seed=1,params=params_test)

```

```{r}
plot(Y_state~time,data=sim1.sim,type='l',col='red',main='Oringinal vs Simulated',ylab='Log returns')
lines(ntd_df)
legend('topright',legend=c("Original","Simulated"),col=c("black","red"),lty = c(1,1))
```

We can see this is a poor simulation, but we will use this parameter set as a start to make a local search later.

```{r}
sim1.filt <- pomp(sim1.sim, 
                  covar=data.frame(
                    covaryt=c(obs(sim1.sim),NA),
                    time=c(timezero(sim1.sim),time(sim1.sim))),
                  tcovar="time",
                  statenames=ntd_statenames,
                  paramnames=ntd_paramnames,
                  covarnames=ntd_covarnames,
                  rprocess=discrete.time.sim(step.fun=Csnippet(ntd_rproc.filt),delta.t=1)
)
```

### 4.2 Filtering on simulated data

I then use this simulation data to estimate a likelibood.
```{r}
run_level <- 3 
ntd_Np <-          c(100,1000,5000)
ntd_Nmif <-        c(10, 100,200)
ntd_Nreps_eval <-  c(4,  10,  20)
ntd_Nreps_local <- c(10, 20, 20)
ntd_Nreps_global <-c(10, 20, 100)


require(doParallel)
registerDoParallel(20)

stew("pf1.rda",{
  t.pf1 <- system.time(
    pf1 <- foreach(i=1:ntd_Nreps_eval[run_level],.packages='pomp',
                   .options.multicore=list(set.seed=TRUE)) %dopar% try(
                     pfilter(sim1.filt,Np=ntd_Np[run_level])
                   )
  )
},seed=493536993,kind="L'Ecuyer")
(L.pf1 <- logmeanexp(sapply(pf1,logLik),se=TRUE))
```
We can see this gives us an unbiased likelihood estimate of -732.7 with a Monte standard error of 0.03.

### 4.3 Fitting the stochastic leverage model

I now use the IF2 algorithm of Ionides et al. to iterated filtering on the data.
```{r}
ntd_rw.sd_rp <- 0.02
ntd_rw.sd_ivp <- 0.1
ntd_cooling.fraction.50 <- 0.5

stew("mif1.rda",{
  t.if1 <- system.time({
    if1 <- foreach(i=1:ntd_Nreps_local[run_level],
                   .packages='pomp', .combine=c,
                   .options.multicore=list(set.seed=TRUE)) %dopar% try(
                     mif2(ntd.filt,
                          start=params_test,
                          Np=ntd_Np[run_level],
                          Nmif=ntd_Nmif[run_level],
                          cooling.type="geometric",
                          cooling.fraction.50=ntd_cooling.fraction.50,
                          transform=TRUE,
                          rw.sd = rw.sd(
                            sigma_nu  = ntd_rw.sd_rp,
                            mu_h      = ntd_rw.sd_rp,
                            phi       = ntd_rw.sd_rp,
                            sigma_eta = ntd_rw.sd_rp,
                            G_0       = ivp(ntd_rw.sd_ivp),
                            H_0       = ivp(ntd_rw.sd_ivp)
                          )
                     )
                   )
    
    L.if1 <- foreach(i=1:ntd_Nreps_local[run_level],.packages='pomp',
                     .combine=rbind,.options.multicore=list(set.seed=TRUE)) %dopar% 
                     {
                       logmeanexp(
                         replicate(ntd_Nreps_eval[run_level],
                                   logLik(pfilter(ntd.filt,params=coef(if1[[i]]),Np=ntd_Np[run_level]))
                         ),
                         se=TRUE)
                     }
  })
},seed=318817883,kind="L'Ecuyer")

r.if1 <- data.frame(logLik=L.if1[,1],logLik_se=L.if1[,2],t(sapply(if1,coef)))
if (run_level>1) 
  write.table(r.if1,file="ntd_params.csv",append=TRUE,col.names=FALSE,row.names=FALSE)
summary(r.if1$logLik,digits=5)
```
As we can see, this gives us a max likelihood 799.0.


```{r}
pairs(~logLik+sigma_nu+mu_h+phi+sigma_eta,data=subset(r.if1,logLik>max(logLik)-20))
```

From the plot of parameters vs Likelihood, we could get a glance at the approximate best parameter values for the max likelihood.

### 4.4 Likelihood maximization using randomized starting values

We are now ready to estimate the parameters based on global search.

We have to first set intinitial values for the parameter search.

Based on the results from the local search in the previous section, we could construct a box containing reasonable parameter values:
```{r}
ntd_box <- rbind(
  sigma_nu=c(0,0.05),
  mu_h    =c(-8,-5),
  phi = c(0,0.5),
  sigma_eta = c(0.8,1),
  G_0 = c(-2,2),
  H_0 = c(-1,1)
)

stew(file="box_eval.rda",{
  t.box <- system.time({
    if.box <- foreach(i=1:ntd_Nreps_global[run_level],.packages='pomp',.combine=c,
                      .options.multicore=list(set.seed=TRUE)) %dopar%  
      mif2(
        if1[[1]],
        start=apply(ntd_box,1,function(x)runif(1,x))
      )
    
    L.box <- foreach(i=1:ntd_Nreps_global[run_level],.packages='pomp',.combine=rbind,
                     .options.multicore=list(set.seed=TRUE)) %dopar% {
                       set.seed(87932+i)
                       logmeanexp(
                         replicate(ntd_Nreps_eval[run_level],
                                   logLik(pfilter(ntd.filt,params=coef(if.box[[i]]),Np=ntd_Np[run_level]))
                         ), 
                         se=TRUE)
                     }
  })
},seed=290860873,kind="L'Ecuyer")


r.box <- data.frame(logLik=L.box[,1],logLik_se=L.box[,2],t(sapply(if.box,coef)))
if(run_level>1) write.table(r.box,file="ntd_params.csv",append=TRUE,col.names=FALSE,row.names=FALSE)
summary(r.box$logLik,digits=5)
```
The best likelihood was 799.4, which is slightly larger than the one from local search.
```{r}
pairs(~logLik+log(sigma_nu)+mu_h+phi+sigma_eta+H_0,data=subset(r.box,logLik>max(logLik)-10))
```

The plot of parameters shows us the parameters that achieve best likelihood, and they are shown below:

```{r}
lik_max=subset(r.box,logLik==max(logLik))
lik_max
```

Now lets fit those parameters into the model and make a simulation:
```{r}
params_test <- c(
  sigma_nu = exp(log(lik_max$sigma_nu)),  
  mu_h = lik_max$mu_h,       
  phi = expit(logit(lik_max$phi)),     
  sigma_eta = exp(log(lik_max$sigma_eta)),
  G_0 = lik_max$G_0,
  H_0=lik_max$H_0
)
sim1.sim <- pomp(ntd.filt, 
                 statenames=ntd_statenames,
                 paramnames=ntd_paramnames,
                 covarnames=ntd_covarnames,
                 rprocess=discrete.time.sim(step.fun=Csnippet(ntd_rproc.sim),delta.t=1)
)

sim1.sim <- simulate(sim1.sim,seed=8,params=params_test)

plot(Y_state~time,data=sim1.sim,type='l',col='red',main='Oringinal vs Simulated',ylab='Log returns')
lines(ntd_df)
legend('topright',legend=c("Original","Simulated"),col=c("black","red"),lty = c(1,1))
```

We can see this simulation is quite good. Although there are some big fluctuation it does not capture, it fits pretty well.

### 5. GARCH Model

We then fitting the data into a GARCH Model as a benchmark:

```{r}
ntd_garch=garch(ntd_df,grad='numerical',trace=FALSE)
logLik(ntd_garch)
```
We see the GARCH(1,1) model gives a max likelihood of 763.9, which is less than that of POMP model. Thus, POMP model is obviously better than it.

### 6. Conclusion

In this project, I fit the Nintendo stock price returns into ARMA(0,0), POMP and GARCH(1,1) model.

ARMA is not a good option, sinces it is not able to capture the volatality of stock returns.
Comparing GARCH with POMP, we see that POMP achieves a larger likelihood. Moreover, since GARCH's parameters do not have clear meanings in reality, we should choose the POMP model over the GARCH model.

### 7. Reference
[1] https://finance.yahoo.com/quote/NTDOY/history?period1=1208836800&period2=1524369600&interval=1wk&filter=history&frequency=1wk

[2] https://ionides.github.io/531w18/

[3] R. Shumway and D. Stoffer ¡°Time Series Analysis and its Applications¡± 4th edition

[4] Bret¨®, C. 2014. On idiosyncratic stochasticity of financial leverage effects. Statistics & Probability Letters 91:20¨C26.